---
layout:     post
title:      "memcache"
subtitle:   " \"缓存中间件\""
date:       2018-11-03 06:00:00
author:     "青乡"
header-img: "img/post-bg-2015.jpg"
catalog: true
tags:
    - cache
---



# 发展史

# 作用

# 替代了什么
基于内存的数据库。  

替代了关系数据库，因为关系数据库太重了，准备工作需要创建数据库、表，操作数据的时候又需要建立连接，连接很耗资源。

memcache就是为了替代关系数据库，准确的说，是**减轻**(不是真的替代)关系数据库的负担，把部分数据放到内存数据库里。

---
什么样的数据适合放到内存数据库？  
1.非持久化数据  
如果需要持久化，就放到关系数据库。

临时存储。不经常变动的数据。实时性要求不高的数据。

2.数据要小  
内存里只放最重要的数据，因为内存一般比较小。

满足上面的条件，就比较适合放内存数据库。


---
应用场景  
1.手机验证码 //也是一种标识id  

2.各种标识id //防止订单重复提交

3.用户id //其实也可以放缓存，但是最佳实践一般是放会话里

# 为什么要用
因为它是内存数据库，比关系数据库更快。

# 怎么用
1.服务器  
c语言写的。  
启动服务器即可。


2.客户端  
多种方式。

可以图形用户界面客户端，  
web，  
jar包 //java

---
java-最常用的开源jar包是XMemCached  


# 应用场景

# 数据类型
一般数据类型都支持。比如：  
字符串  
对象

# 分布式
支持分布式。

---
可以节约内存  
怎么节约？  
？

---
多个节点怎么安装？与web服务器的关系？  
？

---
多个节点，数据是怎么存储的？备份情况是怎么样的？  
1.服务器是分布式的  
memcache的分布式只是服务器的分布式。

2.数据不是分布式的  
每个服务器的数据都不一样，每个服务器只维护自己的数据。如果这个服务器挂了，这个服务器的数据就真的挂了。

那现在数据怎么办？缓存服务器的数据是来自数据库，第一次请求的时候，缓存没有数据，就从数据库取，后面从缓存取。现在缓存挂了数据也没了，这个服务器不可用。如果有新的请求，就又回到第一次请求的时候一样，从数据库取，不过这个时候换了一台缓存服务器，后面的请求就从这个新的缓存服务器取。

怎么知道是否需要换一个新的缓存服务器？有路由算法，专门去选择一个可用的缓存服务器。

总而言之，memcache的数据非分布式，即不是那种所有缓存服务器的数据统一一致，一台缓存服务器数据有更新，会通知其他的所有缓存服务器也同步更新。不是这样。jboss cache是这样，数据是分布式统一管理/统一同步更新。

最后总结，memcache的运行步骤就是：
1.选择一个缓存服务器
路由算法。

2.写数据  
key会经过hash算法，hash后的值包含了目标缓存服务器的唯一标识信息(IP+端口)。

3.读数据  
根据key的hash算法，选择写数据时的同一台服务器读数据。

# 支持编程语言
基本上都支持。

包括：java，c，c++等。

# 底层实现
map数据结构。

# 访问速度
1 //因为是map数据结构

# 并发数量
每秒数百万请求。  

Queries on slow machines should run in well under 1ms. High end servers can serve millions of keys per second in throughput.

# 跑起来玩一下
1.生产环境  
安装在多台服务器  

2.测试环境
安装在同一台服务器，使用不同的端口即可。

# 工作实践
1.用户量小的系统  
用户量  
几十万个用户。

---
节点数量  
服务   
中间件 //多台服务器，集群/分布式。节点数量是个位数。

数据库 //分库，但是每个分库只有一个节点，没有集群。只有数据备份。

内存 //平均每个服务占几G，几十个服务，加起来几百G。

2.用户量大的系统-核心支付系统  
用户量  
几十万个商家。  
数百万数千万的用户。

---
节点数量  
服务   
中间件 //多台服务器，集群/分布式。节点数量是十位数。

数据库 //分库，并且每个分库集群。同地多活，即多机房。有数据备份。

内存 //平均每个服务占几G，几百个服务，加起来几千G。

# 源码分析

# 底层原理和实现

# 参考
https://github.com/memcached/memcached/wiki  
http://www.cnblogs.com/xrq730/p/4954152.html
